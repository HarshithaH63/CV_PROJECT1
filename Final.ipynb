{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c621917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- detection_depth_fusion_final_with_observations.py --------------------\n",
    "import os, time, csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "# -------------------- 1. Config --------------------\n",
    "IMG_PATH = \"./leftImg8bit_trainvaltest/leftImg8bit/val/frankfurt/frankfurt_000000_003025_leftImg8bit.png\"\n",
    "GT_DEPTH_PATH = \"./gt_depth/frankfurt_000000_003025_depth.png\"  # optional\n",
    "OUT_DIR = \"./outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_IMAGE = os.path.join(OUT_DIR, \"annotated.png\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"detections_with_distance.csv\")\n",
    "\n",
    "YOLO_WEIGHTS = \"yolov8n.pt\"\n",
    "OBSTACLE_CLASS_NAMES = {\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\",\n",
    "    \"truck\", \"train\", \"traffic light\", \"stop sign\", \"bench\"\n",
    "}\n",
    "\n",
    "# -------------------- 2. Camera Assumptions --------------------\n",
    "IMAGE_WIDTH = 2048\n",
    "ASSUMED_HORIZONTAL_FOV_DEG = 90.0\n",
    "FOCAL_LENGTH_PX = IMAGE_WIDTH / (2 * np.tan(np.deg2rad(ASSUMED_HORIZONTAL_FOV_DEG / 2)))\n",
    "\n",
    "AVG_HEIGHTS = {\n",
    "    \"person\": 1.7, \"car\": 1.5, \"bus\": 3.0, \"truck\": 3.5,\n",
    "    \"bicycle\": 1.2, \"motorcycle\": 1.2, \"traffic light\": 3.0, \"bench\": 0.8\n",
    "}\n",
    "\n",
    "# -------------------- 3. Helper Functions --------------------\n",
    "def load_image(path):\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    return cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "\n",
    "def run_yolo(img_bgr, weights=YOLO_WEIGHTS, conf=0.25):\n",
    "    model = YOLO(weights)\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = model.predict(source=img_rgb, conf=conf, verbose=False)[0]\n",
    "    detections = []\n",
    "    names = model.model.names\n",
    "    for b in results.boxes:\n",
    "        cls_id = int(b.cls.item())\n",
    "        cls_name = names.get(cls_id, str(cls_id))\n",
    "        if cls_name not in OBSTACLE_CLASS_NAMES: continue\n",
    "        xyxy = b.xyxy.squeeze().cpu().numpy()\n",
    "        conf_score = float(b.conf.item())\n",
    "        detections.append({\"cls_name\": cls_name, \"conf\": conf_score, \"xyxy\": xyxy})\n",
    "    return detections\n",
    "\n",
    "def load_depth_anything(model_id=\"depth-anything/Depth-Anything-V2-small-hf\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    model = AutoModelForDepthEstimation.from_pretrained(model_id).to(device)\n",
    "    model.eval()\n",
    "    return processor, model, device\n",
    "\n",
    "def run_depth_anything(img_bgr, processor, model, device):\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    inputs = processor(images=img_rgb, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth.squeeze().cpu().numpy().astype(np.float32)\n",
    "    return (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min() + 1e-8)\n",
    "\n",
    "def median_depth_in_box(depth_map, xyxy):\n",
    "    x1, y1, x2, y2 = xyxy.astype(int)\n",
    "    h, w = depth_map.shape[:2]\n",
    "    x1, x2 = np.clip([x1, x2], 0, w - 1)\n",
    "    y1, y2 = np.clip([y1, y2], 0, h - 1)\n",
    "    if x2 <= x1 or y2 <= y1: return float(\"nan\")\n",
    "    roi = depth_map[y1:y2, x1:x2]\n",
    "    return float(np.median(roi)) if roi.size > 0 else float(\"nan\")\n",
    "\n",
    "def direction_from_box(xyxy, img_w):\n",
    "    cx = 0.5 * (xyxy[0] + xyxy[2])\n",
    "    if cx < img_w / 3: return \"Left\"\n",
    "    elif cx > 2 * img_w / 3: return \"Right\"\n",
    "    return \"Center\"\n",
    "\n",
    "def approximate_distance(xyxy, cls_name, focal_px=FOCAL_LENGTH_PX):\n",
    "    x1, y1, x2, y2 = xyxy.astype(int)\n",
    "    bbox_height_px = max(1, y2 - y1)\n",
    "    real_height_m = AVG_HEIGHTS.get(cls_name, 1.7)\n",
    "    return (focal_px * real_height_m) / bbox_height_px\n",
    "\n",
    "def annotate_and_save(img_bgr, detections, depth_map, out_image_path, out_csv_path):\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    vis = img_bgr.copy()\n",
    "    with open(out_csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"class\",\"confidence\",\"distance_rel\",\"distance_m\",\"direction\",\"x1\",\"y1\",\"x2\",\"y2\"])\n",
    "        detections_sorted = sorted(detections, key=lambda d: median_depth_in_box(depth_map, d[\"xyxy\"]))\n",
    "        for det in detections_sorted:\n",
    "            dist_rel = median_depth_in_box(depth_map, det[\"xyxy\"])\n",
    "            direction = direction_from_box(det[\"xyxy\"], w)\n",
    "            x1, y1, x2, y2 = det[\"xyxy\"].astype(int)\n",
    "            dist_m = approximate_distance(det[\"xyxy\"], det[\"cls_name\"])\n",
    "            cv2.rectangle(vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            label = f\"{det['cls_name']} {det['conf']:.2f} | rel={dist_rel:.2f} | {dist_m:.1f}m | {direction}\"\n",
    "            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            cv2.rectangle(vis, (x1, max(0,y1-th-6)), (x1+tw+6, y1), (255,255,255), -1)\n",
    "            cv2.putText(vis, label, (x1+3,max(0,y1-4)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1)\n",
    "            writer.writerow([det[\"cls_name\"], f\"{det['conf']:.4f}\", f\"{dist_rel:.4f}\", f\"{dist_m:.2f}\", direction, x1, y1, x2, y2])\n",
    "    cv2.imwrite(out_image_path, vis)\n",
    "    return vis\n",
    "\n",
    "def evaluate_depth(pred_depth, gt_path):\n",
    "    if not os.path.exists(gt_path):\n",
    "        print(\"[INFO] GT depth not found. Skipping depth evaluation.\")\n",
    "        return None\n",
    "    gt = cv2.imread(gt_path, cv2.IMREAD_UNCHANGED).astype(np.float32)\n",
    "    gt /= gt.max()\n",
    "    rmse = np.sqrt(np.mean((pred_depth - gt)**2))\n",
    "    mae = np.mean(np.abs(pred_depth - gt))\n",
    "    print(f\"[METRIC] Depth RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "    return rmse, mae\n",
    "\n",
    "def generate_observations(csv_path, depth_eval=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"\\n--- Quantitative Analysis ---\")\n",
    "    print(\"Total objects detected:\", len(df))\n",
    "    print(\"Detections per class:\\n\", df[\"class\"].value_counts())\n",
    "    print(\"Mean relative depth:\", df[\"distance_rel\"].mean())\n",
    "    print(\"Closest obstacle:\\n\", df.loc[df[\"distance_rel\"].idxmin()])\n",
    "    print(\"Farthest obstacle:\\n\", df.loc[df[\"distance_rel\"].idxmax()])\n",
    "    print(\"Direction distribution:\\n\", df[\"direction\"].value_counts())\n",
    "    if depth_eval is not None:\n",
    "        rmse, mae = depth_eval\n",
    "        print(f\"Optional depth evaluation -> RMSE: {rmse:.4f}, MAE: {mae:.4f}\")\n",
    "\n",
    "    # Scatter plot: confidence vs relative depth\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.scatter(df[\"confidence\"], df[\"distance_rel\"], alpha=0.7)\n",
    "    plt.xlabel(\"YOLO Confidence\")\n",
    "    plt.ylabel(\"Relative Depth (0=near)\")\n",
    "    plt.title(\"Confidence vs Relative Depth\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Mean depth per class\n",
    "    plt.figure(figsize=(10,5))\n",
    "    df.groupby(\"class\")[\"distance_rel\"].mean().sort_values().plot(kind=\"bar\", color=\"skyblue\")\n",
    "    plt.ylabel(\"Mean Relative Depth\")\n",
    "    plt.title(\"Average Distance by Object Class\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -------------------- 4. Main --------------------\n",
    "def main():\n",
    "    print(\"[INFO] Running YOLO + DepthAnything pipeline with calibration & observations...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    img_bgr = load_image(IMG_PATH)\n",
    "    detections = run_yolo(img_bgr)\n",
    "    print(f\"[INFO] {len(detections)} detections found.\")\n",
    "\n",
    "    processor, depth_model, device = load_depth_anything()\n",
    "    depth_rel = run_depth_anything(img_bgr, processor, depth_model, device)\n",
    "\n",
    "    annotated = annotate_and_save(img_bgr, detections, depth_rel, OUT_IMAGE, OUT_CSV)\n",
    "    print(f\"[INFO] Annotated image and CSV saved.\")\n",
    "\n",
    "    # Optional depth evaluation\n",
    "    depth_eval = evaluate_depth(depth_rel, GT_DEPTH_PATH)\n",
    "\n",
    "    # Generate observations and plots\n",
    "    generate_observations(OUT_CSV, depth_eval)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"[INFO] Total pipeline time: {end_time - start_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0791bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
