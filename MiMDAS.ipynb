{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c522977-d03e-4f1f-9924-0cd5cd41b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2c5cdd9-2486-46f6-8f25-eefbbc8f97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = os.environ.get(\"CITYSCAPES_IMG\", \"./leftImg8bit_trainvaltest/leftImg8bit/val/frankfurt/frankfurt_000000_003025_leftImg8bit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "744cf500-9b46-4297-bf80-1c2afee52dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = os.environ.get(\"OUT_DIR\", \"./outputs\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_IMAGE = os.path.join(OUT_DIR, \"annotated.png\")\n",
    "OUT_CSV   = os.path.join(OUT_DIR, \"detections_with_distance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c0d6f02-59d2-4ded-9983-6d86a3db333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_WEIGHTS = os.environ.get(\"YOLO_WEIGHTS\", \"yolov8n.pt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c944dd5-5724-47dc-8463-083e65bd7ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIDAS_MODEL_NAME = os.environ.get(\"MIDAS_MODEL\", \"DPT_Large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9584c662-ee42-4c8a-a65d-142053a9a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBSTACLE_CLASS_NAMES = {\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\", \"train\",\n",
    "    \"traffic light\", \"stop sign\", \"bench\"  # add more if desired\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f6f0b1c-23f4-41ce-afb4-70e47b7a9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path: str) -> np.ndarray:\n",
    "    if not os.path.isfile(path):\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    img_bgr = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img_bgr is None:\n",
    "        raise RuntimeError(f\"Failed to read image: {path}\")\n",
    "    return img_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0066984b-f9ed-4885-9dae-c8236c46a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yolo(img_bgr: np.ndarray, weights: str = YOLO_WEIGHTS, conf: float = 0.25):\n",
    "    \"\"\"\n",
    "    Run YOLOv8 inference on a BGR image.\n",
    "    Returns a list of detections: dict with keys [cls_name, conf, xyxy(np.array)]\n",
    "    \"\"\"\n",
    "    model = YOLO(weights)\n",
    "    # Ultralytics expects path or array in RGB\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    results = model.predict(source=img_rgb, conf=conf, verbose=False)[0]\n",
    "\n",
    "    detections = []\n",
    "    names = model.model.names  # class index -> name\n",
    "    if results.boxes is not None and len(results.boxes) > 0:\n",
    "        for b in results.boxes:\n",
    "            cls_id = int(b.cls.item())\n",
    "            cls_name = names.get(cls_id, str(cls_id))\n",
    "            if cls_name not in OBSTACLE_CLASS_NAMES:\n",
    "                continue\n",
    "            xyxy = b.xyxy.squeeze().cpu().numpy()  # [x1, y1, x2, y2]\n",
    "            conf_score = float(b.conf.item())\n",
    "            detections.append({\n",
    "                \"cls_name\": cls_name,\n",
    "                \"conf\": conf_score,\n",
    "                \"xyxy\": xyxy\n",
    "            })\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "679d7a1f-3a16-447d-92ef-1ac6a0ac396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midas(model_name: str = MIDAS_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Load MiDaS model + transform via torch.hub.\n",
    "    Returns: (model, transform, is_cuda)\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    midas = torch.hub.load(\"isl-org/MiDaS\", model_name)\n",
    "    midas.to(device)\n",
    "    midas.eval()\n",
    "\n",
    "    transforms = torch.hub.load(\"isl-org/MiDaS\", \"transforms\")\n",
    "    if \"DPT\" in model_name:\n",
    "        transform = transforms.dpt_transform\n",
    "    else:\n",
    "        transform = transforms.small_transform\n",
    "    return midas, transform, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84924d8e-d454-4368-b924-d0e48cdad10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_midas_depth(img_bgr: np.ndarray, midas, transform, device: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute relative depth map with MiDaS. Returns a float32 HxW array (larger = farther or nearer depending on model).\n",
    "    We'll invert later if needed for intuition.\n",
    "    \"\"\"\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    input_batch = transform(img_rgb).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        depth = prediction.squeeze().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    # Normalize depth for visualization & consistency\n",
    "    d_min, d_max = float(depth.min()), float(depth.max())\n",
    "    if d_max > d_min:\n",
    "        depth_norm = (depth - d_min) / (d_max - d_min + 1e-8)  # 0..1\n",
    "    else:\n",
    "        depth_norm = np.zeros_like(depth, dtype=np.float32)\n",
    "    return depth_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a02fdf02-2397-43c3-b2a0-28dfc3a1c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_depth_in_box(depth_map: np.ndarray, xyxy: np.ndarray) -> float:\n",
    "    x1, y1, x2, y2 = xyxy.astype(int)\n",
    "    h, w = depth_map.shape[:2]\n",
    "    x1 = np.clip(x1, 0, w-1)\n",
    "    x2 = np.clip(x2, 0, w-1)\n",
    "    y1 = np.clip(y1, 0, h-1)\n",
    "    y2 = np.clip(y2, 0, h-1)\n",
    "    if x2 <= x1 or y2 <= y1:\n",
    "        return float(\"nan\")\n",
    "    roi = depth_map[y1:y2, x1:x2]\n",
    "    if roi.size == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.median(roi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "501bc4b3-a5b2-4c1a-8a3b-36892b4ce233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def direction_from_box(xyxy: np.ndarray, img_w: int) -> str:\n",
    "    x1, y1, x2, y2 = xyxy\n",
    "    cx = 0.5 * (x1 + x2)\n",
    "    left_thr = img_w / 3.0\n",
    "    right_thr = 2.0 * img_w / 3.0\n",
    "    if cx < left_thr:\n",
    "        return \"Left\"\n",
    "    elif cx > right_thr:\n",
    "        return \"Right\"\n",
    "    else:\n",
    "        return \"Center\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e93757ab-3da7-4867-bcf4-70ede22e3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_to_meters(depth_rel: float, a: float = 20.0, b: float = 1.0) -> float:\n",
    "    \"\"\"\n",
    "    Optional naive conversion from relative depth [0,1] to 'meters' using linear scaling.\n",
    "    Tune (a, b) on a small calibration set (e.g., a * depth_rel + b).\n",
    "    Defaults produce arbitrary but monotonic output.\n",
    "    \"\"\"\n",
    "    return float(a * depth_rel + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbf9b2cd-f4d1-42ec-aec1-3fbebc9a3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_and_save(img_bgr: np.ndarray, detections: List[Dict], depth_map: np.ndarray,\n",
    "                      out_image_path: str, out_csv_path: str, convert_to_m: bool = False):\n",
    "    os.makedirs(os.path.dirname(out_image_path), exist_ok=True)\n",
    "\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    vis = img_bgr.copy()\n",
    "\n",
    "    # Prepare CSV\n",
    "    with open(out_csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"class\", \"confidence\", \"distance_rel\", \"distance_m_est\", \"direction\", \"x1\", \"y1\", \"x2\", \"y2\"])\n",
    "\n",
    "        for det in detections:\n",
    "            cls_name = det[\"cls_name\"]\n",
    "            conf = det[\"conf\"]\n",
    "            xyxy = det[\"xyxy\"]\n",
    "            dist_rel = median_depth_in_box(depth_map, xyxy)  # 0..1 (farther = larger with our normalization)\n",
    "            # Intuition: Invert so \"closer\" => larger number if you prefer. We'll keep as-is and label with meters.\n",
    "            direction = direction_from_box(xyxy, w)\n",
    "            if convert_to_m:\n",
    "                dist_m = relative_to_meters(dist_rel)\n",
    "            else:\n",
    "                dist_m = None\n",
    "\n",
    "            # Draw\n",
    "            x1, y1, x2, y2 = xyxy.astype(int)\n",
    "            color = (0, 255, 0)\n",
    "            cv2.rectangle(vis, (x1, y1), (x2, y2), color, 2)\n",
    "            label = f\"{cls_name} {conf:.2f} | {'~'+str(dist_m)[:5]+' m' if dist_m is not None else f'rel={dist_rel:.2f}'} | {direction}\"\n",
    "            (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "            cv2.rectangle(vis, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n",
    "            cv2.putText(vis, label, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "            writer.writerow([cls_name, f\"{conf:.4f}\", f\"{dist_rel:.4f}\", f\"{dist_m:.4f}\" if dist_m is not None else \"\",\n",
    "                             direction, x1, y1, x2, y2])\n",
    "\n",
    "    cv2.imwrite(out_image_path, vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "417a66af-543f-46f0-a33b-8702374507fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    t0 = time.time()\n",
    "    print(\"[INFO] Loading image:\", IMG_PATH)\n",
    "    img_bgr = load_image(IMG_PATH)\n",
    "\n",
    "    print(\"[INFO] Running YOLOv8...\")\n",
    "    detections = run_yolo(img_bgr, weights=YOLO_WEIGHTS, conf=0.25)\n",
    "    print(f\"[INFO] Detections kept (obstacles only): {len(detections)}\")\n",
    "\n",
    "    print(\"[INFO] Loading MiDaS:\", MIDAS_MODEL_NAME)\n",
    "    midas, transform, device = load_midas(MIDAS_MODEL_NAME)\n",
    "\n",
    "    print(\"[INFO] Predicting depth...\")\n",
    "    depth_rel = run_midas_depth(img_bgr, midas, transform, device)  # 0..1\n",
    "\n",
    "    print(\"[INFO] Fusing detection + depth and saving outputs...\")\n",
    "    annotate_and_save(img_bgr, detections, depth_rel, OUT_IMAGE, OUT_CSV, convert_to_m=False)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[DONE] Saved: {OUT_IMAGE}\")\n",
    "    print(f\"[DONE] Saved: {OUT_CSV}\")\n",
    "    print(f\"[TIME] {dt:.2f} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18c9f7fc-19f5-4c8a-9061-aa04f723be2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading image: ./leftImg8bit_trainvaltest/leftImg8bit/val/frankfurt/frankfurt_000000_003025_leftImg8bit.png\n",
      "[INFO] Running YOLOv8...\n",
      "[INFO] Detections kept (obstacles only): 10\n",
      "[INFO] Loading MiDaS: DPT_Large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\harsh/.cache\\torch\\hub\\isl-org_MiDaS_master\n",
      "Using cache found in C:\\Users\\harsh/.cache\\torch\\hub\\isl-org_MiDaS_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Predicting depth...\n",
      "[INFO] Fusing detection + depth and saving outputs...\n",
      "[DONE] Saved: ./outputs\\annotated.png\n",
      "[DONE] Saved: ./outputs\\detections_with_distance.csv\n",
      "[TIME] 43.89 s\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if IMG_PATH == \"./leftImg8bit/val/frankfurt/frankfurt_000000_000294_leftImg8bit.png\":\n",
    "        print(\"[WARNING] Please set IMG_PATH at the top of the script to a real Cityscapes image path.\")\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
